{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "082dce03",
   "metadata": {},
   "source": [
    "- torch.nn.Linear( ) : 입력 x와 가중치 행렬 A간의 곱셈 구현(=feed-forward layer, fully connected layer) <br>\n",
    "<br>\n",
    "     y= x * A**T + b\n",
    "<br>\n",
    "x : 레이어에 대한 입력 (딥러닝은 torch.nn.Linear()와 같은 레이어들이 서로 쌓여 있는 구조) <br>\n",
    "A : 레이어에서 생성된 가중치 행렬 <br>\n",
    "- 이 행렬은 처음에 난수로 시작하며, 신경망이 데이터의 패턴을 더 잘 표현하도록 학습함에 따라 조정됨 (\"T\"는 가중치 행렬이 전치되기 떄문) <br>\n",
    "-참고 : 가중치 행렬을 나타내기 위해 W 또는 X와 같은 문자를 자주 볼 수도 있음 <br>\n",
    "b : 가중치와 입력값을 약간 상쇄하기 위해 사용되는 편향<br>\n",
    "y : 출력 값<br>\n",
    "--> 선형 함수! <br>\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "b4c81083",
   "metadata": {},
   "outputs": [],
   "source": [
    "tensor_A = torch.tensor([[1, 2],\n",
    "                         [3, 4],\n",
    "                         [5, 6]], dtype=torch.float32)\n",
    "\n",
    "tensor_B = torch.tensor([[7, 10],\n",
    "                         [8, 11], \n",
    "                         [9, 12]], dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "8f301ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input shape : torch.Size([3, 2])\n",
      "\n",
      "Output:\n",
      "tensor([[2.2368, 1.2292, 0.4714, 0.3864, 0.1309, 0.9838],\n",
      "        [4.4919, 2.1970, 0.4469, 0.5285, 0.3401, 2.4777],\n",
      "        [6.7469, 3.1648, 0.4224, 0.6705, 0.5493, 3.9716]],\n",
      "       grad_fn=<AddmmBackward0>)\n",
      "\n",
      "Output shape: torch.Size([3, 6])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "#선형 레이어는 무작위 가중치 행렬로 시작하므로 재현 가능하도록 만듬\n",
    "torch.manual_seed(42) #난수 생성의 기준값(seed) 고정\n",
    "#행렬 곱셈 사용\n",
    "linear = torch.nn.Linear(in_features=2, #in_features : 입력 데이터의 특성 \n",
    "                         out_features=6) # out_features = 출력으로 만들 특성 \n",
    "x = tensor_A\n",
    "output = linear (x) #입력 x에 선형 변환을 적용 . (=output = x @ W.T + b)\n",
    "print(f\"Input shape : {x.shape}\\n\")\n",
    "print(f\"Output:\\n{output}\\n\\nOutput shape: {output.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47a00f98",
   "metadata": {},
   "source": [
    "# 최소값, 최대값, 평균, 합계 등 구하는 작업 (집계)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "59ddf11f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0, 10, 20, 30, 40, 50, 60, 70, 80, 90])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a tensor\n",
    "x = torch.arange(0, 100, 10)\n",
    "x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76f2cc6d",
   "metadata": {},
   "source": [
    "집계 작업 수행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "43b2f5e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Minimum: 0\n",
      "Maximum: 90\n",
      "Mean: 45.0\n",
      "Sum: 450\n"
     ]
    }
   ],
   "source": [
    "print(f\"Minimum: {x.min()}\")\n",
    "print(f\"Maximum: {x.max()}\")\n",
    "print(f\"Mean: {x.type(torch.float32).mean()}\") #실수 데이터 타입 아니면 오류\n",
    "print(f\"Sum: {x.sum()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "74cf757e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(90), tensor(0), tensor(45.), tensor(450))"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# torch 메서드 이용\n",
    "torch.max(x), torch.min(x), torch.mean(x.type(torch.float32)), torch.sum(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e3342829",
   "metadata": {},
   "source": [
    "# positional min/max (위치 정보를 포함한 최솟값/최댓값)\n",
    "\n",
    "torch.argmax( ) / torch.argmin( ) : 텐서의 최댓값/최솟값 발생하는 인덱스 찾기 <br>\n",
    "-> 실제 값 자체가 아니라 최대/최소 값이 있는 위치만 필요한 경우에 유용"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f413de46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor: tensor([10, 20, 30, 40, 50, 60, 70, 80, 90])\n",
      "Index where max value occurs: 8\n",
      "Index where min value occurs: 0\n"
     ]
    }
   ],
   "source": [
    "# create a tensor\n",
    "tensor = torch.arange(10, 100, 10)\n",
    "print(f\"Tensor: {tensor}\")\n",
    "\n",
    "#최대, 최소 값의 인덱스 반환\n",
    "print(f\"Index where max value occurs: {tensor.argmax()}\")\n",
    "print(f\"Index where min value occurs: {tensor.argmin()}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7700bfdd",
   "metadata": {},
   "source": [
    "# 텐서의 데이터 타입 변경\n",
    "\n",
    "데이터 타입이 다른 텐서들의 연산 시 오류 발생 ex) torch.float64인 텐서와 torch.float32인 경우 <br>\n",
    "- torch.Tensor.type(dtype=None) : dtype을 매개변수로 데이터 유형을 지정하여 텐서의 데이터타입 변경"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "9c26647b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#텐서 생성 후 데이터타입 확인\n",
    "tensor = torch.arange(10., 100., 10.)\n",
    "tensor.dtype"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "ae3f2bc0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([10, 20, 30, 40, 50, 60, 70, 80, 90], dtype=torch.int8)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#float16 텐서 생성\n",
    "tensor_int8 = tensor.type(torch.int8)\n",
    "tensor_int8\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7d0c605",
   "metadata": {},
   "source": [
    "*숫자가 작을 수록 컴퓨터가 값을 저장하는 정밀도 낮음"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e927a93b",
   "metadata": {},
   "source": [
    "#  Reshaping, stacking, squeezing and unsqueezing\n",
    "\n",
    "텐서 내부 값 변경하지 않고 텐서의 형태나 차원만 변경\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "747d2c42",
   "metadata": {},
   "source": [
    "- torch.reshape(input, shape) : 입력값의 모양을 변경 (호환되는 경우) (=torch.Tensor.reshape())\n",
    "- Tensor.view(shape) : 원본 텐서의 데이터를 다른 shape 으로 표현한 뷰 반환. 이때 원래 텐서와 데이터는 같음.\n",
    "- torch.stack(tensors, dim=0) : 새로운 차원(dim)에 따라 일련의 tensors을 연결. 이때 쌓이는 모든 텐서는 모양이 완전히 같아야 한다.\n",
    "- torch.squeeze(input) : 값이 1인 모든 차원을 제거하기 위해 input를 압축\n",
    "- torch.unsqueeze(input, dim) : 지정한 dim에 크기가 1인 차원을 추가한 input을 반환\n",
    "- torch.permute(input, dims) : 원본 입력 텐서를 지정한 순서(dims)대로 차원을 재배열한 뷰(view)를 반환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "881ab657",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4., 5., 6., 7.]), torch.Size([7]))"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create a tensor\n",
    "import torch\n",
    "x = torch. arange(1., 8.)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "3d8eafc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[1., 2., 3., 4., 5., 6., 7.]]), torch.Size([1, 7]))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_reshaped = x.reshape(1,7)\n",
    "x_reshaped, x_reshaped.shape #add an extra dimension "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "11c88562",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([1., 2., 3., 4., 5., 6., 7.]),\n",
       " tensor([[1., 2., 3., 4., 5., 6., 7.]]),\n",
       " torch.Size([1, 7]))"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Change view - 기존 데이터는 같지만 view 변경\n",
    "z=x.view(1,7)  \n",
    "x, z, z.shape\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "92b58705",
   "metadata": {},
   "source": [
    "- 텐서의 view 변경은 동일한 텐서의 새 view를 생성하는 것 <bt>\n",
    "-> view 변경하면 원래 텐서도 변경됨\n",
    "\n",
    "*view : 같은 텐서를 다른 모양으로 바라보는 것"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "eec4fa57",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[5., 2., 3., 4., 5., 6., 7.]]), tensor([5., 2., 3., 4., 5., 6., 7.]))"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#z의 변화는 x를 바꿈\n",
    "z[:,0] = 5\n",
    "z, x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "9194d165",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[5., 2., 3., 4., 5., 6., 7.],\n",
       "        [5., 2., 3., 4., 5., 6., 7.],\n",
       "        [5., 2., 3., 4., 5., 6., 7.],\n",
       "        [5., 2., 3., 4., 5., 6., 7.]])"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#텐서를 겹쳐 쌓기 \n",
    "x_stacked = torch.stack([x, x, x, x], dim=0) \n",
    "x_stacked"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43295d44",
   "metadata": {},
   "source": [
    "텐서에서 단일 차웜 모두 제거 <br>\n",
    "-> torch.squeeze( ) : 텐서의 차원이 1보다 큰 값만 갖도록 압축"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "2ae833e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\n",
      "Previous shape: torch.Size([1, 7])\n",
      "\n",
      "New tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\n",
      "New shape: torch.Size([7])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Previous tensor: {x_reshaped}\")\n",
    "print(f\"Previous shape: {x_reshaped.shape}\")\n",
    "\n",
    "#x_shaped에서 추가 차원 삭제\n",
    "x_squeezed = x_reshaped.squeeze()\n",
    "print(f\"\\nNew tensor: {x_squeezed}\")\n",
    "print(f\"New shape: {x_squeezed.shape}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d3165efa",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous tensor: tensor([5., 2., 3., 4., 5., 6., 7.])\n",
      "Previous shape: torch.Size([7])\n",
      "\n",
      "New tensor: tensor([[5., 2., 3., 4., 5., 6., 7.]])\n",
      "New shape: torch.Size([1, 7])\n"
     ]
    }
   ],
   "source": [
    "print(f\"Previous tensor: {x_squeezed}\")\n",
    "print(f\"Previous shape: {x_squeezed.shape}\")\n",
    "\n",
    "x_unsqueezed = x_squeezed.unsqueeze(dim=0)  #unsqueeze() : 특정 인덱스에 차원이 1인 값 추가\n",
    "print(f\"\\nNew tensor: {x_unsqueezed}\")  \n",
    "print(f\"New shape: {x_unsqueezed.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5338ff56",
   "metadata": {},
   "source": [
    "torch.permute(input, dims)를 사용하면 축(axis)의 순서를 다시 배치할 수 있는데, 이때 입력 텐서는 새로운 차원 순서를 가진 뷰(view) 로 변환"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbc64596",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Previous shape: torch.Size([224, 224, 3])\n",
      "New shape: torch.Size([3, 224, 224])\n"
     ]
    }
   ],
   "source": [
    "x_original = torch.rand(size=(224, 224, 3))\n",
    "\n",
    "#축 순서를 재배열하기 위해 원래 텐서 순열\n",
    "x_permuted = x_original.permute(2, 0, 1) #shifts axis 0->1, 1->2, 2->0\n",
    "\n",
    "print(f\"Previous shape: {x_original.shape}\")\n",
    "print(f\"New shape: {x_permuted.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d726c04",
   "metadata": {},
   "source": [
    "*순열된 텐서 값은 원본 텐서 값과 동일"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8523fafb",
   "metadata": {},
   "source": [
    "# 인덱싱 (텐서에서 데이터 선택)\n",
    "\n",
    "- 인덱싱 값은 외부차원에서 내부 차원으로 이동\n",
    "- x[batch][row][column]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "283ec910",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[[1, 2, 3],\n",
       "          [4, 5, 6],\n",
       "          [7, 8, 9]]]),\n",
       " torch.Size([1, 3, 3]))"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x = torch.arange(1, 10).reshape(1, 3, 3)\n",
    "x, x.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d17b3b54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "첫 번째 대괄호: \n",
      " tensor([[1, 2, 3],\n",
      "        [4, 5, 6],\n",
      "        [7, 8, 9]]) \n",
      "두 번째 대괄호: tensor([1, 2, 3]) \n",
      "세 번째 대괄호: 1 \n"
     ]
    }
   ],
   "source": [
    "#대괄호 하나씩 출력\n",
    "print ( f\"첫 번째 대괄호: \\n { x [ 0 ] } \" )    #x[0] : 0번 차원(batch)에서 0번째 선택 -> 결과는 (3,3)\n",
    "print ( f\"두 번째 대괄호: { x [ 0 ][ 0 ] } \" )  #batch 안에서 0번째 행(row) 선택\n",
    "print ( f\"세 번째 대괄호: { x [ 0 ][ 0 ][ 0 ] } \" )   #x[0][0]   == x[0, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756aaa0d",
   "metadata": {},
   "source": [
    " 이 차원의 모든 값들을 지정하기 위해 ': ' 사용 후 ' ,'사용해 다른 차원 추가"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "a5368090",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1, 2, 3]])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0번째 차원의 모든 값& 1번째 차원의 0번째 인덱스 가져옴\n",
    "\n",
    "x[:, 0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "eda02741",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[2, 5, 8]])"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#0번째 및 1번째 차원의 모든 값 가져오지만 2번째 차원의 인덱스 1만 가져옴\n",
    "x[:, :, 1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "5a7d586a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([5])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 0차원의 모든 값, 1차원/2차원의 인덱스 값 중 1만 가져옴\n",
    "x[:, 1, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a4428eb",
   "metadata": {},
   "source": [
    "# Pytorch tensor & NumPy\n",
    " NumPy : Numerical Python. 파이썬에서 다차원 배열/행열 연산 라이브러리\n",
    "\n",
    " - NumPy와 Pytorch가 데이터 주고받을 때 사용할 주요 방법\n",
    " 1. torch.from_numpy(ndarray) - Numpy 배열을 Pytorch 텐서로 변환\n",
    " 2. torch.Tensor.numpy()  - Pytorch 텐서를  Numpy 배열로 변환\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "46f5443f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([1., 2., 3., 4., 5., 6., 7.]),\n",
       " tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# NumPy 배열을 tensor로 변환\n",
    "import numpy as np\n",
    "array = np.arange(1.0, 8.0)\n",
    "tensor = torch.from_numpy(array)\n",
    "array, tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d97d4050",
   "metadata": {},
   "source": [
    "*NumPy 배열을 기본적으로 float64 데이터 유형으로 생성됨 (pytorch 텐서로 변환해도 동일한 데이터 유형 유지됨) <br>\n",
    "하지만 대부분 pytorch 연산은 float32 사용 <br>\n",
    "-->   tensor = torch.from_numpy(array).type(torch.float32) :  NumPy 배열(float64)을 PyTorch 텐서(float64)로, 다시 PyTorch 텐서(float32)로 변환\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "3e6f5c18",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(array([2., 3., 4., 5., 6., 7., 8.]),\n",
       " tensor([1., 2., 3., 4., 5., 6., 7.], dtype=torch.float64))"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#배열 변경, 텐서 유지\n",
    "array = array + 1\n",
    "array, tensor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3390d055",
   "metadata": {},
   "source": [
    "# 재현성 (무작위성에서 무작위성을 제거)\n",
    "\n",
    "seed : 무작위성에 영향을 미치는 역할을 하는 정수\n",
    "- torch.manual_seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "6033de7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor C:\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "Tensor D:\n",
      "tensor([[0.8823, 0.9150, 0.3829, 0.9593],\n",
      "        [0.3904, 0.6009, 0.2566, 0.7936],\n",
      "        [0.9408, 0.1332, 0.9346, 0.5936]])\n",
      "\n",
      "Does Tensor C equal Tensor D? (anywhere)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True],\n",
       "        [True, True, True, True],\n",
       "        [True, True, True, True]])"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import random\n",
    "RANDOM_SEED=42  #random seed 설정\n",
    "torch.manual_seed(seed=RANDOM_SEED)\n",
    "random_tensor_C = torch.rand(3,4)\n",
    "#새 rand() 호출될 때마다 seed 리셋해야 함(안그럼 tensor_c랑 tensor_D랑 다를 수 있음)\n",
    "torch.random.manual_seed(seed=RANDOM_SEED) #이 줄 주석처리 해보기 --> 주석처리하면 다르게 됨\n",
    "random_tensor_D = torch.rand(3,4)\n",
    "\n",
    "print(f\"Tensor C:\\n{random_tensor_C}\\n\")\n",
    "print(f\"Tensor D:\\n{random_tensor_D}\\n\")\n",
    "print(f\"Does Tensor C equal Tensor D? (anywhere)\")\n",
    "random_tensor_C == random_tensor_D"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15e4a355",
   "metadata": {},
   "source": [
    "# GPU에서 텐서 실행해 계산 속도 향상\n",
    "\n",
    "-수치 연산은 기본적으로 CPU(중앙 처리 장치)에서 수행됨 <br>\n",
    "-GPU(그래픽 처리 장치) - 신경망에 필요한 특정 연산(행렬 곱셈)을 수행하는 데 있어 CPU보다 훨씬 빠른 경우가 있음 <br>\n",
    "-> 신경망을 훈련시킬 때 가능한 자주 사용해야 훈련 시간을 획기적으로 딘축시켜줌"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd774931",
   "metadata": {},
   "source": [
    "1. pytorch를 GPU에서 실행"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "cdd2a3a6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# pytorch가 GPU에 접근할 수 있는지 확인\n",
    "torch.cuda.is_available()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "8bbf7b88",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'cuda'"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#device 타입 설정\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3afee2d",
   "metadata": {},
   "source": [
    "\"cuda\" --> 모든 PyTorch 코드가 사용 가능한 CUDA 장치(GPU)를 사용하도록 설정할 수 있다는 의미\n",
    "* PyTorch에서는 디바이스에 구애받지 않는 코드를 작성하는 것이 best <br>\n",
    "-> CPU(항상 사용 가능) 또는 GPU(사용 가능한 경우)에서 모두 실행될 수 있는 코드를 작성\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9ac76b45",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.cuda.device_count() #pytorch가 접근할 수 있는 GPU 장치 개수 세기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d44a87f4",
   "metadata": {},
   "source": [
    "*PyTorch가 사용할 수 있는 GPU 개수를 아는 것 <br>\n",
    " - >  특정 프로세스를 하나의 GPU에서 실행하고 다른 프로세스를 다른 GPU에서 실행하려는 경우에 유용"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eb3a8d",
   "metadata": {},
   "source": [
    "2. 텐서(및 모델)를 GPU에 배치\n",
    "- to (device) : 텐서( 및 모델)을 특정 장치에 배치     *device:텐서를 배치할 대상 장치\n",
    "\n",
    "이렇게 하는 이유 -> GPU를 사용할 수 없는 경우 장치에 구애받지 않는 코드 덕분에 CPU에서 실행됨\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "id": "157fa7bf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1, 2, 3]) cpu\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 72,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 텐서 생성 (기본값 : CPU)\n",
    "tensor = torch.tensor([1, 2, 3])\n",
    "\n",
    "# 텐서가 GPU에 없는 것 확인\n",
    "print(tensor, tensor.device)\n",
    "\n",
    "#텐서를 GPU로 이동 (가능한 경우)\n",
    "tensor_on_gpu = tensor.to(device)\n",
    "tensor_on_gpu\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "935ee947",
   "metadata": {},
   "source": [
    "device='cuda:0' -> 사용 가능한 0번째 GPU (GPU는 0부터 시작, 만약 GPU가 2개면 cuda:0과 cuda:1 있을것임. cuda:n까지 )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "deea89b0",
   "metadata": {},
   "source": [
    "3. 텐서를 CPU로 다시 이동\n",
    "\n",
    "-numpy를 사용해 텐서와 사용히려는 경우 텐서를 CPU로 옮기는 작업 수행 <br>\n",
    "-tensor_on_gpu,numpy() 사용시 에러 <br>\n",
    "- tensor.cpu( ) : 텐서를 CPU로 다시 가져와 NumPy에서 사용할 수 있게 함. <br>\n",
    "-텐서를 CPU 메모리로 복사하여 CPU에서 사용가능하게 함.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e77f08bb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_back_on_cpu = tensor_on_gpu.cpu().numpy()\n",
    "tensor_back_on_cpu      #이 때 GPU 텐서의 복사본을 CPU 메모리에 반환해 원본 텐서는 여전히 GPU에 있음"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "id": "9e1a0a7e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], device='cuda:0')"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tensor_on_gpu"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45aa2769",
   "metadata": {},
   "source": [
    "# 01. PyTorch 워크플로우 기본 사항\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95af075",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "abc",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
